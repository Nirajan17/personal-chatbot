{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# mistral = ChatOllama(\n",
    "#     model=\"mistral\",\n",
    "#     temperature=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# embeddings = OllamaEmbeddings(\n",
    "#     model=\"mistral\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using Groqcloud API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groqLLM = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-qwen-32b\", \n",
    "    api_key=api_key,\n",
    "    temperature=0.7, \n",
    "    max_tokens=512    # Limit response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings.embed_query(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:00<00:00,  7.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "def custom_loader(file_path: str):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "loader = DirectoryLoader(\"personal\", glob=\"**/*\", show_progress=True, loader_cls=custom_loader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_retreiver = vector_store.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path:str) -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "tools = [marks_reader]\n",
    "groqLLM_with_tools = groqLLM.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a digital copy of this user that can provide information based his documents on both vector search from a database and other tools.\n",
    "\n",
    "    ### Task:\n",
    "    You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\n",
    "    2. If additional context or information is required that can be retrieved from tools, use the appropriate tool.\n",
    "    3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "    ### User Query:\n",
    "    {user_query}\n",
    "\n",
    "    ### Relevant Documents (from Pinecone vector database):\n",
    "    {retrieved_documents}\n",
    "\n",
    "    ### Tool Usage:\n",
    "    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the marks.txt file.\n",
    "    If you used any tools, describe how the tool was used in your response.\n",
    "\n",
    "    ### Answer:\n",
    "    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | groqLLM_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 370, 'total_tokens': 394, 'completion_time': 0.171428571, 'prompt_time': 0.021585505, 'queue_time': 0.053641697, 'total_time': 0.193014076}, 'model_name': 'deepseek-r1-distill-qwen-32b', 'system_fingerprint': 'fp_d458a8aba5', 'finish_reason': 'stop', 'logprobs': None}, id='run-f641d736-2b41-4b33-98b7-7ce44146db46-0', usage_metadata={'input_tokens': 370, 'output_tokens': 24, 'total_tokens': 394})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"user_query\": \"Hello how are you\",\n",
    "              \"retrieved_documents\": \"there are no documents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = input(\"Your query :: \\n\")\n",
    "# retrieved_documents = mistral_retriever.invoke(user_query)\n",
    "\n",
    "# chain = prompt | mistral_with_tools\n",
    "\n",
    "# response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The name of the candidate mentioned in the provided document is Nabaraj Subedi.\n"
     ]
    }
   ],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add memory to the chatbot using **langgraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import operator\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    user_query : str\n",
    "    retrieved_documents : Sequence[str]\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def input_node(state: GraphState) -> GraphState:\n",
    "    if not state.get(\"messages\"):\n",
    "        state[\"messages\"] = [HumanMessage(content=state[\"user_query\"])]\n",
    "    return state\n",
    "\n",
    "def retrieval_node(state: GraphState)->GraphState:\n",
    "    retrieved_documents = groq_retreiver.invoke(state['user_query'])\n",
    "    state['retrieved_documents'] = retrieved_documents\n",
    "    return state\n",
    "\n",
    "def processing_node(state: GraphState)->GraphState:\n",
    "    chain = prompt | groqLLM_with_tools\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"user_query\": state['user_query'],\n",
    "            \"retrieved_documents\": state['retrieved_documents']\n",
    "        }\n",
    "    )\n",
    "    print(f\"reponse of the processing node {response}\")\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"retrieval\", retrieval_node)\n",
    "workflow.add_node(\"processing\", processing_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\",\"retrieval\")\n",
    "workflow.add_edge(\"retrieval\",\"processing\")\n",
    "\n",
    "workflow.add_conditional_edges(\"processing\", tools_condition)  # if tool call is there, it is called\n",
    "workflow.add_edge(\"tools\", \"processing\")  # if toolcall is made then tool execution should occur, if occur, this is ..\n",
    "\n",
    "workflow.add_edge(\"processing\",END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reponse of the processing node content=\"The user's name is **NIRAJAN PAUDEL**, as clearly stated in the resume document. There's no need to use the `marks_reader` tool since the name is already provided in the available documents.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 1054, 'total_tokens': 1248, 'completion_time': 1.385714286, 'prompt_time': 0.063373553, 'queue_time': 0.053424325999999994, 'total_time': 1.449087839}, 'model_name': 'deepseek-r1-distill-qwen-32b', 'system_fingerprint': 'fp_0852292947', 'finish_reason': 'stop', 'logprobs': None} id='run-c9c24764-8485-4207-8fd7-d0206cacdb10-0' usage_metadata={'input_tokens': 1054, 'output_tokens': 194, 'total_tokens': 1248}\n",
      "The user's name is **NIRAJAN PAUDEL**, as clearly stated in the resume document. There's no need to use the `marks_reader` tool since the name is already provided in the available documents.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"You:: \\n\")\n",
    "initial_state = {\n",
    "    \"user_query\": user_query,\n",
    "    \"retrieved_documents\": [],\n",
    "    \"messages\": []\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "result = app.invoke(initial_state,config=config)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'what are my marks in C programming?',\n",
       " 'retrieved_documents': [Document(id='c955d3f5-2d9c-4cd6-90d8-f4f5017310b0', metadata={'page': 0.0, 'source': 'personal/IELTS result.pdf'}, page_content='Centre Number\\nSex (M/F)\\nDate Test Report \\nForm Number\\nCEFR \\nLevel\\nDate\\nTest Results\\nValidation stamp\\nIELTs· \\n□ □ \\nee BRITISH \\n••coUNCIL \\n□ \\nJ..idp \\n□ \\nL □ \\n,,,,f,. Cambridge Assessment \\n::: English \\n□ \\nScheme Code\\nCandidate Details\\nFamily Name\\nFirst Name(s)\\nCandidate ID\\nOverall\\nBand\\n Score\\nRecognising \\norganisations must \\nverify this score at \\nielts.org/verify\\nAdministrator Comments\\nReading Writing SpeakingListening\\nDate of Birth\\nCountry or \\nRegion of Origin\\nCountry of \\nNationality\\nFirst Language\\nAdmission to undergraduate and post graduate courses should be based on the ACADEMIC Reading and Writing Modules. \\nGENERAL TRAINING Reading and Writing Modules are not designed to test the full range of language skills required for academic purposes. \\nIt is recommended that the candidate’s language ability as indicated in this Test Report Form be re-assessed after two years from the date of the test. \\nTo find out more about IELTS, IELTS band scores and the CEFR levels, please visit ielts.org/scores\\nTest Report Form\\nNOTE\\nCandidate Number\\nACADEMIC\\n23NP502241PAUN004A\\n03/JAN/2024NP004\\n18/02/2000\\n502241\\nPAUDEL\\nNIRAJAN\\n12607688\\nM Private Candidate\\nNEPAL\\nNEPALI\\n8.09.0 8.0 7.0 7.0 C1\\n06/01/2024\\n'),\n",
       "  Document(id='0f4a6469-d1b4-42fa-8c78-ec3c1c77f98a', metadata={'page': 3.0, 'source': 'personal/Resume.pdf'}, page_content=' PROGRAM  AND  PARTICIPATIONS   Datathon             2024  Regional  competition  conducted  by  ICES,  Pashchimanchal  Campus  First  position     COURSES  AND  CERTIFICATIONS   Advanced  Learning  Algorithms  Deeplearning.ai  Coursera  \\n●\\n \\nBuild  and  train  a  neural  network  with  TensorFlow  to  perform  multi-class  classification  \\n●\\n \\nApply  best  practices  for  machine  learning  development  so  that  your  models  generalize  to  data  and  tasks  in  the  real  \\nworld\\n \\n●\\n \\nBuild  and  use  decision  trees  and  tree  ensemble  methods,  including  random  forests  and  boosted  trees   Supervised  Machine  Learning:  Regression  and  Classification  Deeplearning.ai  Coursera  \\n●\\n \\nBuild  machine  learning  models  in  Python  using  popular  machine  learning  libraries  NumPy  &  scikit-learn  \\n●\\n \\nBuild  &  train  supervised  machine  learning  models  for  prediction  &  binary  classification  tasks,  including  linear  \\nregression\\n \\n&\\n \\nlogistic\\n \\nregression\\n ')],\n",
       " 'messages': [HumanMessage(content='what are my marks in C programming?', additional_kwargs={}, response_metadata={}, id='2a460451-5ae7-40ae-8ed2-85578fdbd262'),\n",
       "  AIMessage(content='<function name=\"marks_reader\" arguments=\"{\\'file_path\\': \\'marks.txt\\'}\"/>', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 279, 'prompt_tokens': 1183, 'total_tokens': 1462, 'completion_time': 1.9928571430000002, 'prompt_time': 0.058696535, 'queue_time': 0.060618408000000006, 'total_time': 2.051553678}, 'model_name': 'deepseek-r1-distill-qwen-32b', 'system_fingerprint': 'fp_d458a8aba5', 'finish_reason': 'stop', 'logprobs': None}, id='run-f4a39008-5655-4656-a138-21320819d3d2-0', usage_metadata={'input_tokens': 1183, 'output_tokens': 279, 'total_tokens': 1462})]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
