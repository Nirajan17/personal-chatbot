{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# mistral = ChatOllama(\n",
    "#     model=\"mistral\",\n",
    "#     temperature=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# embeddings = OllamaEmbeddings(\n",
    "#     model=\"mistral\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using Groqcloud API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groqLLM = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-qwen-32b\", \n",
    "    api_key=api_key,\n",
    "    temperature=0.7, \n",
    "    max_tokens=512    # Limit response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings.embed_query(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "def custom_loader(file_path: str):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "loader = DirectoryLoader(\"personal\", glob=\"**/*\", show_progress=True, loader_cls=custom_loader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_retreiver = vector_store.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path:str) -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "tools = [marks_reader]\n",
    "groqLLM_with_tools = groqLLM.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a digital copy of this user that can provide information based on his documents using both vector search from a database and other tools.\n",
    "\n",
    "### Task:\n",
    "You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "1. Retrieve relevant information from the vector database (Pinecone) that best matches the query, provided in the 'Relevant Documents' section.\n",
    "2. If additional context or information is required beyond the retrieved documents, use the appropriate tool (e.g., marks_reader for reading marks from files).\n",
    "3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "### User Query:\n",
    "{user_query}\n",
    "\n",
    "### Relevant Documents (from Pinecone vector database):\n",
    "{retrieved_documents}\n",
    "\n",
    "### Instructions:\n",
    "- If the retrieved documents are sufficient, respond directly with a concise, clear, and comprehensive answer.\n",
    "- If the documents are insufficient and a tool is needed (e.g., to fetch marks from 'marks.txt'), invoke the appropriate tool using the tool-calling protocol. Do not output raw JSON; instead, use the tool and include its result in your response.\n",
    "- If you use a tool, briefly explain in your response how it was used (e.g., \"I used the marks_reader tool to fetch your marks from marks.txt\").\n",
    "\n",
    "### Answer:\n",
    "Provide your response here, integrating information from the retrieved documents and any tool results.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "#     template=template,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | groqLLM_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"user_query\": \"Hello how are you\",\n",
    "              \"retrieved_documents\": \"there are no documents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = input(\"Your query :: \\n\")\n",
    "# retrieved_documents = mistral_retriever.invoke(user_query)\n",
    "\n",
    "# chain = prompt | mistral_with_tools\n",
    "\n",
    "# response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add memory to the chatbot using **langgraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import operator\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    user_query : str\n",
    "    retrieved_documents : Sequence[str]\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def input_node(state: GraphState) -> GraphState:\n",
    "    if not state.get(\"messages\"):\n",
    "        state[\"messages\"] = [HumanMessage(content=state[\"user_query\"])]\n",
    "    return state\n",
    "\n",
    "def retrieval_node(state: GraphState)->GraphState:\n",
    "    retrieved_documents = groq_retreiver.invoke(state['user_query'])\n",
    "    state['retrieved_documents'] = retrieved_documents\n",
    "    return state\n",
    "\n",
    "def processing_node(state: GraphState)->GraphState:\n",
    "    chain = prompt | groqLLM_with_tools\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"user_query\": state['user_query'],\n",
    "            \"retrieved_documents\": state['retrieved_documents']\n",
    "        }\n",
    "    )\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"retrieval\", retrieval_node)\n",
    "workflow.add_node(\"processing\", processing_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\",\"retrieval\")\n",
    "workflow.add_edge(\"retrieval\",\"processing\")\n",
    "\n",
    "workflow.add_conditional_edges(\"processing\", tools_condition)  # if tool call is there, it is called\n",
    "workflow.add_edge(\"tools\", \"processing\")  # if toolcall is made then tool execution should occur, if occur, this is ..\n",
    "\n",
    "workflow.add_edge(\"processing\",END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "\n",
    "print(\"Start chatting! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    initial_state = {\n",
    "        \"user_query\": user_query,\n",
    "        \"retrieved_documents\": [],\n",
    "        \"messages\": app.get_state(config).values.get(\"messages\", [])  # Load previous messages\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(initial_state, config=config)\n",
    "    print(\"Assistant:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
