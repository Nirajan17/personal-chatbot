{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "mistral = ChatOllama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mistral\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=4096,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_retriever = vector_store.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path: str=\"nirajan_transcript.txt\") -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "tools = [marks_reader]\n",
    "mistral_with_tools = mistral.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful personal assistant of this user that can provide information based on both vector search from a database and other tools.\n",
    "\n",
    "    ### Task:\n",
    "    You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\n",
    "    2. If additional context or information is required that can be retrieved from tools, use the appropriate tool.\n",
    "    3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "    ### User Query:\n",
    "    {user_query}\n",
    "\n",
    "    ### Relevant Documents (from Pinecone vector database):\n",
    "    {retrieved_documents}\n",
    "\n",
    "    ### Tool Usage:\n",
    "    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\n",
    "    If you used any tools, describe how the tool was used in your response.\n",
    "\n",
    "    ### Answer:\n",
    "    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | mistral_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = input(\"Your query :: \\n\")\n",
    "# retrieved_documents = mistral_retriever.invoke(user_query)\n",
    "\n",
    "# chain = prompt | mistral_with_tools\n",
    "\n",
    "# response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The name of the candidate mentioned in the provided document is Nabaraj Subedi.\n"
     ]
    }
   ],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add memory to the chatbot using **langgraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    user_query : str\n",
    "    retrieved_documents : Sequence[str]\n",
    "    response : str\n",
    "\n",
    "def input_node(state: GraphState) -> GraphState:\n",
    "    return state\n",
    "\n",
    "def retrieval_node(state: GraphState)->GraphState:\n",
    "    retrieved_documents = mistral_retriever.invoke(state['user_query'])\n",
    "    state['retrieved_documents'] = retrieved_documents\n",
    "    return state\n",
    "\n",
    "def processing_node(state: GraphState)->GraphState:\n",
    "    chain = prompt | mistral_with_tools\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"user_query\": state['user_query'],\n",
    "            \"retrieved_documents\": state['retrieved_documents']\n",
    "        }\n",
    "    )\n",
    "    state['response'] = response.content\n",
    "    return state\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"retrieval\", retrieval_node)\n",
    "workflow.add_node(\"processing\", processing_node)\n",
    "\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\",\"retrieval\")\n",
    "workflow.add_edge(\"retrieval\",\"processing\")\n",
    "workflow.add_edge(\"processing\",END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the documents I have retrieved, it appears that there is no direct question or answer related to your previous question in the provided text files. However, if you are referring to a specific topic or subject discussed in these documents, I can provide more information about them.\n",
      "\n",
      "   For instance, one of the documents discusses a paper titled \"Bornon: Bengali image captioning with transformer-based deep learning approach\" by Muhammad Shah et al., published in SN Computer Science 3 (2022). If you are referring to this paper or its topic, I can provide more details about it.\n",
      "\n",
      "   To get more specific information, I could use the marks_reader tool to search for any personal documents that might contain information related to your previous question. However, since no such documents were provided in the context of this query, I will not use the marks_reader tool at this time. If you have a specific document or topic in mind, please let me know so I can help you find more information about it.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"YOU:: \\n\")\n",
    "initial_state = {\"user_query\": user_query}\n",
    "result = app.invoke(initial_state,config=config)\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
