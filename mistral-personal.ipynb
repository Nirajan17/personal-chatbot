{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# mistral = ChatOllama(\n",
    "#     model=\"mistral\",\n",
    "#     temperature=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# embeddings = OllamaEmbeddings(\n",
    "#     model=\"mistral\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using Groqcloud API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groqLLM = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-qwen-32b\", \n",
    "    api_key=api_key,\n",
    "    temperature=0.7, \n",
    "    max_tokens=512    # Limit response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings.embed_query(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "def custom_loader(file_path: str):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "loader = DirectoryLoader(\"personal\", glob=\"**/*\", show_progress=True, loader_cls=custom_loader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_retreiver = vector_store.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path:str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a specified text file and returns it as a string.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file if successfully read, or an error message if the file is not found or an exception occurs.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist at the specified path.\n",
    "        Exception: For other file-reading errors (e.g., permission issues, encoding errors), with details in the returned string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "@tool\n",
    "def write_to_file(file_path: str, content: str, append: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Write or append content to a specified file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "        content (str): The content to write into the file.\n",
    "        append (bool): If True, appends to the file; if False, overwrites it.\n",
    "    \n",
    "    Returns:\n",
    "        str: Confirmation message or error if it fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mode = \"a\" if append else \"w\"\n",
    "        with open(file_path, mode, encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        return f\"Successfully {'appended to' if append else 'wrote to'} {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file: {str(e)}\"\n",
    "    \n",
    "    \n",
    "tools = [marks_reader, write_to_file]\n",
    "groqLLM_with_tools = groqLLM.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are digitalME, an AI assistant created by Berojgar & company, designed to engage in natural, helpful conversations. You can answer general questions, provide insights, and assist with tasks. When relevant, you have access to a vector database of documents and tools to enhance your responses.\n",
    "\n",
    "### Chat History:\n",
    "{messages}\n",
    "\n",
    "### User Query:\n",
    "{user_query}\n",
    "\n",
    "### Retrieved Documents (from vector database, if applicable):\n",
    "{retrieved_documents}\n",
    "\n",
    "### Instructions:\n",
    "- Engage in a natural, conversational tone as a helpful assistant.\n",
    "- Use the chat history to maintain context and provide coherent, relevant responses.\n",
    "- If the user query can be answered directly based on general knowledge or chat history, do so concisely and clearly.\n",
    "- If the query relates to information in the retrieved documents, incorporate that information into your response and briefly note that it came from the documents (e.g., \"Based on your documents...\").\n",
    "- If the query requires additional information or functionality (e.g., reading a file or writing to a file), use the available tools, execute them fully, and include their results in your response. Do NOT output raw JSON tool calls; instead, summarize the tool's output (e.g., \"I used the file_reader tool to check your sports from sports.txt, and here's what I found: [result]\", or I have succesfully written [text] in the file [file_name]).\n",
    "- If a tool returns an error (e.g., file not found), report it clearly in the response (e.g., \"I tried using the file_reader tool, but the file sports.txt wasnâ€™t found\", or [file_name couldn't be created to write]).\n",
    "- If no retrieved documents or tools are relevant, proceed with a general conversational response.\n",
    "- Keep your answers friendly, concise, and tailored to the user's intent.\n",
    "\n",
    "### Response:\n",
    "Provide your conversational response here, blending general knowledge, document insights, and tool outputs as needed.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | groqLLM_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = input(\"Your query :: \\n\")\n",
    "# retrieved_documents = mistral_retriever.invoke(user_query)\n",
    "\n",
    "# chain = prompt | mistral_with_tools\n",
    "\n",
    "# response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add memory to the chatbot using **langgraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import operator\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    user_query : str\n",
    "    retrieved_documents : Sequence[str]\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def input_node(state: GraphState) -> GraphState:\n",
    "    if not state.get(\"messages\"):\n",
    "        state[\"messages\"] = [HumanMessage(content=state[\"user_query\"])]\n",
    "    return state\n",
    "\n",
    "def retrieval_node(state: GraphState)->GraphState:\n",
    "    retrieved_documents = groq_retreiver.invoke(state['user_query'])\n",
    "    state['retrieved_documents'] = [doc.page_content for doc in retrieved_documents]\n",
    "    return state\n",
    "\n",
    "def processing_node(state: GraphState)->GraphState:\n",
    "    chain = prompt | groqLLM_with_tools\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"user_query\": state['user_query'],\n",
    "            \"retrieved_documents\": state['retrieved_documents']\n",
    "        }\n",
    "    )\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"retrieval\", retrieval_node)\n",
    "workflow.add_node(\"processing\", processing_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\",\"retrieval\")\n",
    "workflow.add_edge(\"retrieval\",\"processing\")\n",
    "\n",
    "workflow.add_conditional_edges(\"processing\", tools_condition)  # if tool call is there, it is called\n",
    "workflow.add_edge(\"tools\", \"processing\")  # if toolcall is made then tool execution should occur, if occur, this is ..\n",
    "\n",
    "workflow.add_edge(\"processing\",END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting! Type 'exit' to quit.\n",
      "Assistant: \n",
      "Assistant: I'll help you read the contents of the `output.txt` file. I used the `marks_reader` tool to read the file, and here's the content:\n",
      "\n",
      "```\n",
      "xxxxxx\n",
      "```\n",
      "\n",
      "If you need any further assistance, feel free to ask!\n",
      "Assistant: I used the `marks_reader` tool to read the content of the `output.txt` file. Here's the content:\n",
      "\n",
      "```\n",
      "xxxxxx\n",
      "```\n",
      "\n",
      "If you need any further assistance, feel free to ask!\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "\n",
    "print(\"Start chatting! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    initial_state = {\n",
    "        \"user_query\": user_query,\n",
    "        \"retrieved_documents\": [],\n",
    "        \"messages\": app.get_state(config).values.get(\"messages\", [])  # Load previous messages\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(initial_state, config=config)\n",
    "    print(\"Assistant:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
