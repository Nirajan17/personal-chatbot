{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "mistral = ChatOllama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mistral\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=4096,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_retriever = vector_store.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path: str=\"nirajan_transcript.txt\") -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "tools = [marks_reader]\n",
    "mistral_with_tools = mistral.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful personal assistant of this user that can provide information based on both vector search from a database and other tools.\n",
    "\n",
    "    ### Task:\n",
    "    You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\n",
    "    2. If additional context or information is required that can be retrieved from tools, use the appropriate tool.\n",
    "    3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "    ### User Query:\n",
    "    {user_query}\n",
    "\n",
    "    ### Relevant Documents (from Pinecone vector database):\n",
    "    {retrieved_documents}\n",
    "\n",
    "    ### Tool Usage:\n",
    "    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\n",
    "    If you used any tools, describe how the tool was used in your response.\n",
    "\n",
    "    ### Answer:\n",
    "    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | mistral_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = input(\"Your query :: \\n\")\n",
    "# retrieved_documents = mistral_retriever.invoke(user_query)\n",
    "\n",
    "# chain = prompt | mistral_with_tools\n",
    "\n",
    "# response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The name of the candidate mentioned in the provided document is Nabaraj Subedi.\n"
     ]
    }
   ],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add memory to the chatbot using **langgraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    user_query : str\n",
    "    retrieved_documents : Sequence[str]\n",
    "    response : str\n",
    "\n",
    "def input_node(state: GraphState) -> GraphState:\n",
    "    return state\n",
    "\n",
    "def retrieval_node(state: GraphState)->GraphState:\n",
    "    retrieved_documents = mistral_retriever.invoke(state['user_query'])\n",
    "    state['retrieved_documents'] = retrieved_documents\n",
    "    return state\n",
    "\n",
    "def processing_node(state: GraphState)->GraphState:\n",
    "    chain = prompt | mistral_with_tools\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"user_query\": state['user_query'],\n",
    "            \"retrieved_documents\": state['retrieved_documents']\n",
    "        }\n",
    "    )\n",
    "    state['response'] = response.content\n",
    "    return state\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"retrieval\", retrieval_node)\n",
    "workflow.add_node(\"processing\", processing_node)\n",
    "\n",
    "workflow.add_edge(START, \"input\")\n",
    "workflow.add_edge(\"input\",\"retrieval\")\n",
    "workflow.add_edge(\"retrieval\",\"processing\")\n",
    "workflow.add_edge(\"processing\",END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the documents available, it appears that you are currently working as a Faculty Lecturer at Western Region Campus (Pashchimanchal Campus) in Pokhara, Nepal. Your subjects include C programming and Information System. You have also had industrial experience as a Virtual Machine learning Intern at MeriSKILL, India where you worked on data analysis techniques and explored report making techniques using tools like Excel. Additionally, you attended the Gandaki University International Conference (GUIC) in 2024 and the Conference on Recent Trends in Science, Technology and Innovation (RTSTI) also in 2024. You have also published papers in the Journal of Soft Computing Paradigm and the Journal of IoT in Social, Mobile, Analytics, and Cloud.\n",
      "\n",
      "   I could not find any information about your current activities or tasks from the documents provided. If you need help with something specific like checking marks for certain subjects, please let me know and I can use the marks_reader tool to assist you.\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\"user_query\": \"what am i currenlty doing?\"}\n",
    "result = app.invoke(initial_state)\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
