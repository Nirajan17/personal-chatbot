{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      " 33%|███▎      | 2/6 [04:16<08:33, 128.27s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:00<00:00,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "def custom_loader(file_path: str):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "loader = DirectoryLoader(\"personal\", glob=\"**/*\", show_progress=True, loader_cls=custom_loader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 37}},\n",
       " 'total_vector_count': 37}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='27d5e11e-10cc-47e0-ae7d-9dff0f711bee', metadata={'page': 9.0, 'source': 'personal/paper-crash-detection.pdf'}, page_content='                                                                                                    Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\nJournal of IoT in Social, Mobile, Analytics, and Cloud, March 2024, Volume 6, Issue 1 63 \\n \\nThe facial detection system accurately identifies yawning and eye conditions as in \\nFigure 10, and the crash false notification triggering mechanism responds as depicted in Figure \\n7. \\n Conclusion \\nThe results obtained from the drowsiness and crash detection system demonstrate its \\neffectiveness in improving road safety. By detecting driver drowsiness and accurately \\nidentifying crash events while promptly notifying relevant parties, the system facilit ates swift \\nemergency response and aids in preventing potential accidents by monitoring the driver\\'s facial \\ncondition. Moreover, it promotes seamless coordination between vehicles and rescue center. \\nThe system\\'s accuracy rate and response time underscore its potential to enhance the efficiency \\nof emergency services and potentially reduce the severity of injuries in crash incidents. \\nNevertheless, it is important to acknowledge certain challenges and limitations. \\nEnvironmental factors, such as adverse weather conditions or heavy traffic congestion, may \\nintroduce additional complexities in crash detection. Further research efforts, such  as \\nintegrating both crash detection and drowsiness detection into a single device, are necessary to \\nenhance the system\\'s performance in such challenging scenarios. \\nReferences \\n[1] F. Bhatti, M. A. Shah, C. Maple, and S. U. Islam, \"A Novel Internet of Things-Enabled \\nAccident Detection and Reporting System for Smart City Environments,\" Sensors, vol. \\n19, no. 9, p. 2071, May 2019, doi: 10.3390/s19092071. \\n[2] S. Uma and R. Eswari, \"Accident prevention and safety assistance using IOT and \\nmachine learning,\" J. Reliab. Intell. Environ., vol. 8, no. 2, pp. 79–103, Jun. 2022, doi: \\n10.1007/s40860-021-00136-3. \\n[3] Pokhrel, Anisha, Laxmi Mahara, Monika Upadhyaya, Shikshya Shrestha, and Badri Raj \\nLamichhane. \"Drowsy Driver Detection with Crash Alert Mechanism using Arduino \\nand Image Processing.\" Journal of Soft Computing Paradigm 5, no. 2 (2023): 194-217. \\n[4] C. Prabha, R. Sunitha, and R. Anitha, \"Automatic Vehicle Accident Detection and \\nMessaging System Using GSM and GPS Modem,\" Int. J. Adv. Res. Electr. Electron. '),\n",
       " Document(id='7568c437-0989-4a54-9270-dd2765cfe68b', metadata={'page': 5.0, 'source': 'personal/paper-crash-detection.pdf'}, page_content=\"                                                                                                    Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\nJournal of IoT in Social, Mobile, Analytics, and Cloud, March 2024, Volume 6, Issue 1 59 \\n \\nFigure 3. Crash Detection Flow Chart \\n3.3 Components Used \\na.   Arduino UNO  \\nThe Arduino Uno, built around the ATmega328P microcontroller, receives velocity and \\ntilt values from the accelerometer [7]. It verifies if these values meet predefined conditions \\nbefore proceeding with further operations. \\nb.   MPU 6050 \\nIn this study, a 10 DOF IMU Sensor, capable of measuring 3 -axis accelerometer data, \\nis utilized to detect changes in a vehicle's orientation, indicative of potential accidents. To \\ncalculate velocity and prevent accidents, the acceleration -time relationship is  leveraged, \\nenabling accurate velocity estimation based on acceleration values over time \\n\"),\n",
       " Document(id='05c32ba2-8b72-4d5f-a672-7bbc6be99748', metadata={'page': 11.0, 'source': 'personal/paper-image-captioning.pdf'}, page_content=\"                                                                                                       Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\n \\nJournal of Soft Computing Paradigm, Month 2024, Volume 6, Issue 1  81 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 9. Sample Result 3 \\n \\n \\n \\n \\n \\n \\n \\nFigure 10. Sample Result 4 \\nThe transformer model correctly predicted the object, environment, relations and the \\nactions in the images with coherent sentences. \\n Conclusion \\nIn conclusion, our research work introduces a novel approach for generating detailed \\nNepali paragraphs to describe images, leveraging both visual and linguistic struct ures. The \\ntransformer's capacity to model long range dependencies on caption to effectively describe the \\n\"),\n",
       " Document(id='1e1423b9-049e-4f7d-a57d-70a02819a2b6', metadata={'page': 2.0, 'source': 'personal/paper-image-captioning.pdf'}, page_content='Nepali Image Captioning: Generating Coherent Paragraph-Length Descriptions Using Transformer \\nISSN: 2582-2640  72 \\n \\n \\nThe key contributions of this research work are: \\n1. We compiled the Nepali Paragraph dataset for image captioning by manually refining \\ncaptions from the English Stanford dataset [1] and creating 800 original Nepali cultural \\nimage descriptions, while also verifying the accuracy of Google -translated content \\nthrough human correction. \\n2. Utilize a Transformer -CNN architecture to generate Nepali Paragraph caption s from \\nimages. Through our research, we aim to contribute to the advancement of image \\ncaptioning technology in Nepali, paving the way for improved accessibility, tourism \\nexperiences, and urban development initiatives in Nepal and beyond. \\n Related Works \\nUtilizing computer vision algorithms for image captioning has primarily been focused \\non English language datasets largely due to the inherent complexities of other languages. \\nHowever, the increasing need for multilingual image captioning has prompted researche rs to \\nexplore the extension of these techniques to languages beyond English. This expansion not \\nonly facilitates image-text retrieval but also enables image captioning and translation in diverse \\nlinguistic contexts. \\nAmong the foundational techniques used i n image captioning is the Long Short -Term \\nMemory (LSTM) neural network [8], renowned for its ability to maintain long -short term \\nmemory, thereby addressing the short -term memory limitations of standard Recurrent Neural \\nNetworks (RNNs). This feature is part icularly crucial for various tasks such as Natural \\nLanguage Processing (NLP), object detection, and machine translation. The prevailing \\nsequence translation models typically employ advanced convolutional and recurrent neural \\nnetworks organized in an encode r-decoder setup, often drawing inspiration from machine \\ntranslation methodologies. \\nIn the domain of non-English language image captioning, significant strides have been \\nmade, particularly in languages such as Hindi and Bengali, which share similarities wit h \\nNepali. In Bengali language research, notable studies by S. Paul et al. [9] have explored \\ntechniques utilizing convolutional neural networks (CNNs) and recurrent neural networks \\n(RNNs) to generate Bengali captions from images. Subsequent investigations b y the same ')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"gre\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tool for pdf reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pypdf import PdfReader\n",
    "# from langchain_core.tools import tool\n",
    "\n",
    "# @tool\n",
    "# def marks_reader(pdf_path: str):\n",
    "#     \"\"\"\n",
    "#     Read the marks of subjects from the pdf and return subject marks\n",
    "#     \"\"\"\n",
    "#     pdf_reader = PdfReader(pdf_path)\n",
    "#     text = \"\"\n",
    "    \n",
    "#     for page_num in range(len(pdf_reader.pages)):\n",
    "#         text += pdf_reader.pages[page_num].extract_text()\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text mark reader tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path: str) -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [marks_reader]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-10T15:35:52.216445Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1705163208, 'load_duration': 37503375, 'prompt_eval_count': 177, 'prompt_eval_duration': 700000000, 'eval_count': 22, 'eval_duration': 964000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-285bafbe-fd71-452e-b19b-dcbad821fbbf-0', tool_calls=[{'name': 'marks_reader', 'args': {'file_path': 'your_first_year_marks.txt'}, 'id': 'a3e4bf09-9b42-41dc-9c58-42ad34fe1a83', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 22, 'total_tokens': 199})"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"can you tell me my first year marks using the markreader tool?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['retrieved_documents', 'user_query'] input_types={} partial_variables={} template='\\n    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\\n\\n    ### Task:\\n    You will be provided with a user query. Your goal is to respond to the query by doing the following:\\n    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\\n    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\\n    3. Combine the information from the database and tools to provide a well-structured and complete response.\\n\\n    ### User Query:\\n    {user_query}\\n\\n    ### Relevant Documents (from Pinecone vector database):\\n    {retrieved_documents}\\n\\n    ### Tool Usage:\\n    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\\n    If you used any tools, describe how the tool was used in your response.\\n\\n    ### Answer:\\n    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\\n '\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\n",
    "\n",
    "    ### Task:\n",
    "    You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\n",
    "    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\n",
    "    3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "    ### User Query:\n",
    "    {user_query}\n",
    "\n",
    "    ### Relevant Documents (from Pinecone vector database):\n",
    "    {retrieved_documents}\n",
    "\n",
    "    ### Tool Usage:\n",
    "    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\n",
    "    If you used any tools, describe how the tool was used in your response.\n",
    "\n",
    "    ### Answer:\n",
    "    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "    template=template,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['retrieved_documents', 'user_query'], input_types={}, partial_variables={}, template='\\n    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\\n\\n    ### Task:\\n    You will be provided with a user query. Your goal is to respond to the query by doing the following:\\n    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\\n    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\\n    3. Combine the information from the database and tools to provide a well-structured and complete response.\\n\\n    ### User Query:\\n    {user_query}\\n\\n    ### Relevant Documents (from Pinecone vector database):\\n    {retrieved_documents}\\n\\n    ### Tool Usage:\\n    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\\n    If you used any tools, describe how the tool was used in your response.\\n\\n    ### Answer:\\n    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\\n ') middle=[] last=RunnableBinding(bound=ChatOllama(model='llama3.2', temperature=0.0), kwargs={'tools': [{'type': 'function', 'function': {'name': 'marks_reader', 'description': 'Read content from a text file.', 'parameters': {'properties': {'self': {}, 'file_path': {'type': 'string'}}, 'required': ['self', 'file_path'], 'type': 'object'}}}]}, config={}, config_factories=[])\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm_with_tools\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided document, I can infer that the author is working on a project related to crash detection for vehicles. The text mentions \"Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane\" as authors and \"Journal of IoT in Social, Mobile, Analytics, and Cloud\" as the publication.\n",
      "\n",
      "The document also discusses the use of Arduino Uno and MPU 6050 components in the project. The author mentions that the Arduino Uno receives velocity and tilt values from the accelerometer and verifies if these values meet predefined conditions before proceeding with further operations.\n",
      "\n",
      "Unfortunately, I couldn't find any information on marks or grades related to specific subjects in the provided document. If you could provide more context or clarify what you're looking for, I'd be happy to try and assist you further.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Your query :: \\n\")\n",
    "retrieved_documents = doc_retriever.invoke(user_query)\n",
    "\n",
    "response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's try with deepseek model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(\n",
    "    model=\"deepseek-r1\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nHi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1', 'created_at': '2025-03-10T16:09:19.630264Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10288325625, 'load_duration': 645679042, 'prompt_eval_count': 8, 'prompt_eval_duration': 3571000000, 'eval_count': 42, 'eval_duration': 6067000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-219ec175-17e1-461f-ada6-44e4dc6df53a-0', usage_metadata={'input_tokens': 8, 'output_tokens': 42, 'total_tokens': 50})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek.invoke(\"which model are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [1:44:19<4:20:49, 3129.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-large-en-v1.5', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='150ee64b-47c0-4b00-b6bd-2c53858c1f70', metadata={'page': 2.0, 'source': 'personal/cu-sop.pdf'}, page_content='knowledge  and  skills  needed  to  make  significant  contributions  to  this  field.  I  look  \\nforward\\n \\nto\\n \\nbringing\\n \\nmy\\n \\nunique\\n \\nperspective,\\n \\ndedication,\\n \\nand\\n \\nenthusiasm\\n \\nto\\n \\nthe\\n \\nUniversity,\\n \\nand\\n \\nI\\n \\nam\\n \\nexcited\\n \\nabout\\n \\nthe\\n \\nopportunities\\n \\nto\\n \\ncollaborate\\n \\nwith\\n \\nlike-minded\\n \\nindividuals\\n \\nand\\n \\ndistinguished\\n \\nprofessors.\\n \\nUpon\\n \\ncompleting\\n \\nmy\\n \\ngraduate\\n \\nstudies,\\n \\nI\\n \\naspire\\n \\nto\\n \\nreturn\\n \\nto\\n \\nNepal\\n \\nand\\n \\ncontribute\\n \\nto\\n \\nthe\\n \\nadvancement\\n \\nof\\n \\nAI\\n \\ntechnology,\\n \\nparticularly\\n \\nin\\n \\napplications\\n \\nthat\\n \\nenhance\\n \\naccessibility\\n \\nand\\n \\nimprove\\n \\nthe\\n \\nquality\\n \\nof\\n \\nlife\\n \\nfor\\n \\nindividuals\\n \\nwith\\n \\ndisabilities.\\n   Here  is  the  link  of  published  articles  :  Nepali  Image  Captioning:  Generating  Coherent  Paragraph-Length  Descriptions  Using  \\nTransformer\\n DOI:  https://doi.org/10.36548/jscp.2024.1.006   Drowsiness  and  Crash  Detection  Mobile  Application  for  Vehicle’s  Safety  DOI:  https://doi.org/10.36548/jismac.2024.1.005         ')]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"statement of purpose\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_deepseek = vector_store.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path: str) -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "    \n",
    "tools = [marks_reader]\n",
    "deepseek_with_tools = deepseek.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\n",
    "\n",
    "    ### Task:\n",
    "    You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\n",
    "    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\n",
    "    3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "    ### User Query:\n",
    "    {user_query}\n",
    "\n",
    "    ### Relevant Documents (from Pinecone vector database):\n",
    "    {retrieved_documents}\n",
    "\n",
    "    ### Tool Usage:\n",
    "    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\n",
    "    If you used any tools, describe how the tool was used in your response.\n",
    "\n",
    "    ### Answer:\n",
    "    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['retrieved_documents', 'user_query'], input_types={}, partial_variables={}, template='\\n    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\\n\\n    ### Task:\\n    You will be provided with a user query. Your goal is to respond to the query by doing the following:\\n    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\\n    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\\n    3. Combine the information from the database and tools to provide a well-structured and complete response.\\n\\n    ### User Query:\\n    {user_query}\\n\\n    ### Relevant Documents (from Pinecone vector database):\\n    {retrieved_documents}\\n\\n    ### Tool Usage:\\n    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\\n    If you used any tools, describe how the tool was used in your response.\\n\\n    ### Answer:\\n    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\\n ') middle=[] last=ChatOllama(model='deepseek-r1', temperature=0.0)\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | deepseek\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to help this user. They provided a chunk of text from a document about someone's application to CU Boulder for an AI program. The user is asking me to analyze their journey into AI, focusing on their motivation, key research themes, and how they plan to contribute.\n",
      "\n",
      "First, I'll read through the text carefully. It starts with a paragraph image captioning thesis, which caught my attention because that's exactly what the user is working on. They mention collaborating with Professor Danna Gurari and the Image and Video Computing Group. That seems important for their research focus.\n",
      "\n",
      "Next, there's a section about their application motivation. They were inspired by an experience with visually impaired individuals in Nepal using Envision glasses, which only work in English. This led them to want to develop solutions in Nepali, making it more accessible. That's a strong motivator and shows the real-world impact they're aiming for.\n",
      "\n",
      "Then, there's a part about their journey being driven by curiosity and desire to maximize AI's positive impact. They mention collaborating with Katharina von der Wense and the NALA group, which is another key point. This indicates they have mentors or groups that support their academic growth.\n",
      "\n",
      "I also notice they're committed to acquiring necessary skills during their graduate studies at CU Boulder. That shows a clear plan for development and aligns with their research themes.\n",
      "\n",
      "So, putting it all together, I need to structure the answer by highlighting their motivation from the Nepalese blind community's needs, their key research themes (Visual Interpretation for Blind People), how they plan to contribute through collaboration and skill acquisition, and their overall commitment to AI's impact.\n",
      "\n",
      "I should make sure each point is clear and concise, using bullet points or sections if needed. Also, since I can't use any tools here, I'll just rely on the information provided in the text.\n",
      "</think>\n",
      "\n",
      "The user's journey into AI is driven by a deep commitment to accessibility for the Nepalese blind community. Their research theme, \"Visual Interpretation for Blind People,\" focuses on leveraging AI to enhance visual communication for individuals who cannot use standard technologies like glasses. This aligns with their undergraduate thesis, which also aimed to support Nepal's blind population.\n",
      "\n",
      "The user is motivated by an inspiring experience involving visually impaired Nepalese individuals using Envision glasses, which only function in English. This led them to focus on developing solutions in Nepali, aiming to improve accessibility for the Nepalese blind community. Their application to CU Boulder was bolstered by their passion for AI's potential to positively impact humanity and the robust academic programs and faculty there.\n",
      "\n",
      "Key research themes include advancing visual interpretation for blind individuals within the Image and Video Computing Group under Professor Danna Gurari, as well as collaborations with the NALA group, led by Katharina von der Wense. The user is committed to acquiring necessary skills during their graduate studies at CU Boulder to achieve these goals.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Your query :: \\n\")\n",
    "retrieved_documents = retriever_deepseek.invoke(user_query)\n",
    "\n",
    "response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
