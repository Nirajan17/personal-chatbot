{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      " 33%|███▎      | 2/6 [04:16<08:33, 128.27s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:00<00:00,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "def custom_loader(file_path: str):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "loader = DirectoryLoader(\"personal\", glob=\"**/*\", show_progress=True, loader_cls=custom_loader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02d79a62-2ade-4b53-bae9-e6d87658715f',\n",
       " '11e12d41-20f7-4753-b57c-f8fee1075ec7',\n",
       " '0a11923c-581a-49a4-9b75-7859d9ec00b8',\n",
       " 'bef2c41e-baec-428c-aa6e-672e610145fc',\n",
       " '4143bec2-ab8b-438f-98eb-9384eff683d0',\n",
       " 'eafaffa6-5a82-465e-8705-b41ac74c74ef',\n",
       " 'd3c6df86-0fc9-487d-8e95-53dc5e7711db',\n",
       " '50ac5db7-b769-422d-a2e4-9bfaf76ac7d0',\n",
       " '1e1423b9-049e-4f7d-a57d-70a02819a2b6',\n",
       " 'b868497c-42c8-4156-8208-fa386d6f4214',\n",
       " 'feb7dc5c-e2ef-415f-8f67-8c6f6374f6e5',\n",
       " 'e231a055-be74-46df-b70a-a3ece848c62d',\n",
       " '0414128a-aed4-4e9f-a84c-a8e8c13067bc',\n",
       " 'e922f8e5-4df5-471b-add1-57cac10f0e96',\n",
       " 'dbb2329e-ebe1-4e01-98be-563b70cfe29f',\n",
       " '6982ebbe-63e1-47fa-9699-ce3bd6781751',\n",
       " '423ea964-b455-4d0a-8461-3e9cb2dbaa83',\n",
       " '05c32ba2-8b72-4d5f-a672-7bbc6be99748',\n",
       " 'e1646ed0-d662-4a70-a806-7ec65359f1c1',\n",
       " 'af0fe243-7b18-4c34-8f64-11138ba5611b',\n",
       " '11d73459-d5f6-4ab7-acb6-dc2db9da5ace',\n",
       " 'e7f6c93d-1bf4-49dc-b60c-c5650d2a60f2',\n",
       " '6f73ec67-969a-4ce6-a8a5-d64ed36e95d5',\n",
       " '05e515a2-bd0b-4d8e-a699-513e067d54b0',\n",
       " '3b0857dc-d254-47ef-9d9a-cc6740d7503c',\n",
       " 'b6247e89-a942-47b3-97ef-5c35f6e45eec',\n",
       " '9929188e-722b-4a7d-b29a-73bcc7f8508e',\n",
       " 'f6cf3c88-8fbb-403d-a7f7-d9a4ed1d906f',\n",
       " '5e07dbec-b5bd-4882-a48b-16bf6735374f',\n",
       " '7568c437-0989-4a54-9270-dd2765cfe68b',\n",
       " 'eb22c803-01f2-4376-bfe6-54605a369164',\n",
       " 'b5600a99-fc9e-4233-b888-e99508c3d505',\n",
       " 'd179fd51-dcb5-4c48-91f6-81bc5fb05c41',\n",
       " '27d5e11e-10cc-47e0-ae7d-9dff0f711bee',\n",
       " 'e6a59693-0edd-423f-bee7-470a1fad014e',\n",
       " 'edef6a5d-aad2-4584-9fb0-8e10529f55e0',\n",
       " 'c9a65590-529c-4762-86bf-e75b830fc101']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 37}},\n",
       " 'total_vector_count': 37}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='27d5e11e-10cc-47e0-ae7d-9dff0f711bee', metadata={'page': 9.0, 'source': 'personal/paper-crash-detection.pdf'}, page_content='                                                                                                    Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\nJournal of IoT in Social, Mobile, Analytics, and Cloud, March 2024, Volume 6, Issue 1 63 \\n \\nThe facial detection system accurately identifies yawning and eye conditions as in \\nFigure 10, and the crash false notification triggering mechanism responds as depicted in Figure \\n7. \\n Conclusion \\nThe results obtained from the drowsiness and crash detection system demonstrate its \\neffectiveness in improving road safety. By detecting driver drowsiness and accurately \\nidentifying crash events while promptly notifying relevant parties, the system facilit ates swift \\nemergency response and aids in preventing potential accidents by monitoring the driver\\'s facial \\ncondition. Moreover, it promotes seamless coordination between vehicles and rescue center. \\nThe system\\'s accuracy rate and response time underscore its potential to enhance the efficiency \\nof emergency services and potentially reduce the severity of injuries in crash incidents. \\nNevertheless, it is important to acknowledge certain challenges and limitations. \\nEnvironmental factors, such as adverse weather conditions or heavy traffic congestion, may \\nintroduce additional complexities in crash detection. Further research efforts, such  as \\nintegrating both crash detection and drowsiness detection into a single device, are necessary to \\nenhance the system\\'s performance in such challenging scenarios. \\nReferences \\n[1] F. Bhatti, M. A. Shah, C. Maple, and S. U. Islam, \"A Novel Internet of Things-Enabled \\nAccident Detection and Reporting System for Smart City Environments,\" Sensors, vol. \\n19, no. 9, p. 2071, May 2019, doi: 10.3390/s19092071. \\n[2] S. Uma and R. Eswari, \"Accident prevention and safety assistance using IOT and \\nmachine learning,\" J. Reliab. Intell. Environ., vol. 8, no. 2, pp. 79–103, Jun. 2022, doi: \\n10.1007/s40860-021-00136-3. \\n[3] Pokhrel, Anisha, Laxmi Mahara, Monika Upadhyaya, Shikshya Shrestha, and Badri Raj \\nLamichhane. \"Drowsy Driver Detection with Crash Alert Mechanism using Arduino \\nand Image Processing.\" Journal of Soft Computing Paradigm 5, no. 2 (2023): 194-217. \\n[4] C. Prabha, R. Sunitha, and R. Anitha, \"Automatic Vehicle Accident Detection and \\nMessaging System Using GSM and GPS Modem,\" Int. J. Adv. Res. Electr. Electron. '),\n",
       " Document(id='7568c437-0989-4a54-9270-dd2765cfe68b', metadata={'page': 5.0, 'source': 'personal/paper-crash-detection.pdf'}, page_content=\"                                                                                                    Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\nJournal of IoT in Social, Mobile, Analytics, and Cloud, March 2024, Volume 6, Issue 1 59 \\n \\nFigure 3. Crash Detection Flow Chart \\n3.3 Components Used \\na.   Arduino UNO  \\nThe Arduino Uno, built around the ATmega328P microcontroller, receives velocity and \\ntilt values from the accelerometer [7]. It verifies if these values meet predefined conditions \\nbefore proceeding with further operations. \\nb.   MPU 6050 \\nIn this study, a 10 DOF IMU Sensor, capable of measuring 3 -axis accelerometer data, \\nis utilized to detect changes in a vehicle's orientation, indicative of potential accidents. To \\ncalculate velocity and prevent accidents, the acceleration -time relationship is  leveraged, \\nenabling accurate velocity estimation based on acceleration values over time \\n\"),\n",
       " Document(id='05c32ba2-8b72-4d5f-a672-7bbc6be99748', metadata={'page': 11.0, 'source': 'personal/paper-image-captioning.pdf'}, page_content=\"                                                                                                       Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\n \\nJournal of Soft Computing Paradigm, Month 2024, Volume 6, Issue 1  81 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 9. Sample Result 3 \\n \\n \\n \\n \\n \\n \\n \\nFigure 10. Sample Result 4 \\nThe transformer model correctly predicted the object, environment, relations and the \\nactions in the images with coherent sentences. \\n Conclusion \\nIn conclusion, our research work introduces a novel approach for generating detailed \\nNepali paragraphs to describe images, leveraging both visual and linguistic struct ures. The \\ntransformer's capacity to model long range dependencies on caption to effectively describe the \\n\"),\n",
       " Document(id='1e1423b9-049e-4f7d-a57d-70a02819a2b6', metadata={'page': 2.0, 'source': 'personal/paper-image-captioning.pdf'}, page_content='Nepali Image Captioning: Generating Coherent Paragraph-Length Descriptions Using Transformer \\nISSN: 2582-2640  72 \\n \\n \\nThe key contributions of this research work are: \\n1. We compiled the Nepali Paragraph dataset for image captioning by manually refining \\ncaptions from the English Stanford dataset [1] and creating 800 original Nepali cultural \\nimage descriptions, while also verifying the accuracy of Google -translated content \\nthrough human correction. \\n2. Utilize a Transformer -CNN architecture to generate Nepali Paragraph caption s from \\nimages. Through our research, we aim to contribute to the advancement of image \\ncaptioning technology in Nepali, paving the way for improved accessibility, tourism \\nexperiences, and urban development initiatives in Nepal and beyond. \\n Related Works \\nUtilizing computer vision algorithms for image captioning has primarily been focused \\non English language datasets largely due to the inherent complexities of other languages. \\nHowever, the increasing need for multilingual image captioning has prompted researche rs to \\nexplore the extension of these techniques to languages beyond English. This expansion not \\nonly facilitates image-text retrieval but also enables image captioning and translation in diverse \\nlinguistic contexts. \\nAmong the foundational techniques used i n image captioning is the Long Short -Term \\nMemory (LSTM) neural network [8], renowned for its ability to maintain long -short term \\nmemory, thereby addressing the short -term memory limitations of standard Recurrent Neural \\nNetworks (RNNs). This feature is part icularly crucial for various tasks such as Natural \\nLanguage Processing (NLP), object detection, and machine translation. The prevailing \\nsequence translation models typically employ advanced convolutional and recurrent neural \\nnetworks organized in an encode r-decoder setup, often drawing inspiration from machine \\ntranslation methodologies. \\nIn the domain of non-English language image captioning, significant strides have been \\nmade, particularly in languages such as Hindi and Bengali, which share similarities wit h \\nNepali. In Bengali language research, notable studies by S. Paul et al. [9] have explored \\ntechniques utilizing convolutional neural networks (CNNs) and recurrent neural networks \\n(RNNs) to generate Bengali captions from images. Subsequent investigations b y the same ')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"gre\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tool for pdf reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pypdf import PdfReader\n",
    "# from langchain_core.tools import tool\n",
    "\n",
    "# @tool\n",
    "# def marks_reader(pdf_path: str):\n",
    "#     \"\"\"\n",
    "#     Read the marks of subjects from the pdf and return subject marks\n",
    "#     \"\"\"\n",
    "#     pdf_reader = PdfReader(pdf_path)\n",
    "#     text = \"\"\n",
    "    \n",
    "#     for page_num in range(len(pdf_reader.pages)):\n",
    "#         text += pdf_reader.pages[page_num].extract_text()\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text mark reader tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(self, file_path: str) -> str:\n",
    "    \"\"\"Read content from a text file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: File not found at path {file_path}\"\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [marks_reader]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-10T15:35:52.216445Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1705163208, 'load_duration': 37503375, 'prompt_eval_count': 177, 'prompt_eval_duration': 700000000, 'eval_count': 22, 'eval_duration': 964000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-285bafbe-fd71-452e-b19b-dcbad821fbbf-0', tool_calls=[{'name': 'marks_reader', 'args': {'file_path': 'your_first_year_marks.txt'}, 'id': 'a3e4bf09-9b42-41dc-9c58-42ad34fe1a83', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 22, 'total_tokens': 199})"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"can you tell me my first year marks using the markreader tool?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['retrieved_documents', 'user_query'] input_types={} partial_variables={} template='\\n    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\\n\\n    ### Task:\\n    You will be provided with a user query. Your goal is to respond to the query by doing the following:\\n    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\\n    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\\n    3. Combine the information from the database and tools to provide a well-structured and complete response.\\n\\n    ### User Query:\\n    {user_query}\\n\\n    ### Relevant Documents (from Pinecone vector database):\\n    {retrieved_documents}\\n\\n    ### Tool Usage:\\n    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\\n    If you used any tools, describe how the tool was used in your response.\\n\\n    ### Answer:\\n    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\\n '\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\n",
    "\n",
    "    ### Task:\n",
    "    You will be provided with a user query. Your goal is to respond to the query by doing the following:\n",
    "    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\n",
    "    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\n",
    "    3. Combine the information from the database and tools to provide a well-structured and complete response.\n",
    "\n",
    "    ### User Query:\n",
    "    {user_query}\n",
    "\n",
    "    ### Relevant Documents (from Pinecone vector database):\n",
    "    {retrieved_documents}\n",
    "\n",
    "    ### Tool Usage:\n",
    "    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\n",
    "    If you used any tools, describe how the tool was used in your response.\n",
    "\n",
    "    ### Answer:\n",
    "    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"retrieved_documents\"],\n",
    "    template=template,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['retrieved_documents', 'user_query'], input_types={}, partial_variables={}, template='\\n    You are a helpful assistant that can provide information based on both vector search from a database and other tools.\\n\\n    ### Task:\\n    You will be provided with a user query. Your goal is to respond to the query by doing the following:\\n    1. Retrieve relevant information from the vector database (Pinecone) that best matches the query.\\n    2. If additional context or information is required that can be retrieved from tools (such as reading a PDF), use the appropriate tool.\\n    3. Combine the information from the database and tools to provide a well-structured and complete response.\\n\\n    ### User Query:\\n    {user_query}\\n\\n    ### Relevant Documents (from Pinecone vector database):\\n    {retrieved_documents}\\n\\n    ### Tool Usage:\\n    If the relevant documents or context are not sufficient to answer the query, use the tools you have. For example, if the user is asking for marks of certain subjects, use the marks_reader tool to extract information from the document.\\n    If you used any tools, describe how the tool was used in your response.\\n\\n    ### Answer:\\n    Your answer should be concise, clear, and comprehensive, using the retrieved documents and any tool-assisted information.\\n ') middle=[] last=RunnableBinding(bound=ChatOllama(model='llama3.2', temperature=0.0), kwargs={'tools': [{'type': 'function', 'function': {'name': 'marks_reader', 'description': 'Read content from a text file.', 'parameters': {'properties': {'self': {}, 'file_path': {'type': 'string'}}, 'required': ['self', 'file_path'], 'type': 'object'}}}]}, config={}, config_factories=[])\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm_with_tools\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided document, I can infer that the author is working on a project related to crash detection for vehicles. The text mentions \"Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane\" as authors and \"Journal of IoT in Social, Mobile, Analytics, and Cloud\" as the publication.\n",
      "\n",
      "The document also discusses the use of Arduino Uno and MPU 6050 components in the project. The author mentions that the Arduino Uno receives velocity and tilt values from the accelerometer and verifies if these values meet predefined conditions before proceeding with further operations.\n",
      "\n",
      "Unfortunately, I couldn't find any information on marks or grades related to specific subjects in the provided document. If you could provide more context or clarify what you're looking for, I'd be happy to try and assist you further.\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Your query :: \\n\")\n",
    "retrieved_documents = doc_retriever.invoke(user_query)\n",
    "\n",
    "response = chain.invoke({\"user_query\": user_query, \"retrieved_documents\": retrieved_documents})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
