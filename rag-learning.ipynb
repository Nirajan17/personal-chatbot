{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      " 33%|███▎      | 2/6 [04:16<08:33, 128.27s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      " 86%|████████▌ | 6/7 [00:00<00:00,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "def custom_loader(file_path: str):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "loader = DirectoryLoader(\"personal\", glob=\"**/*\", show_progress=True, loader_cls=custom_loader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"personal\" \n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02d79a62-2ade-4b53-bae9-e6d87658715f',\n",
       " '11e12d41-20f7-4753-b57c-f8fee1075ec7',\n",
       " '0a11923c-581a-49a4-9b75-7859d9ec00b8',\n",
       " 'bef2c41e-baec-428c-aa6e-672e610145fc',\n",
       " '4143bec2-ab8b-438f-98eb-9384eff683d0',\n",
       " 'eafaffa6-5a82-465e-8705-b41ac74c74ef',\n",
       " 'd3c6df86-0fc9-487d-8e95-53dc5e7711db',\n",
       " '50ac5db7-b769-422d-a2e4-9bfaf76ac7d0',\n",
       " '1e1423b9-049e-4f7d-a57d-70a02819a2b6',\n",
       " 'b868497c-42c8-4156-8208-fa386d6f4214',\n",
       " 'feb7dc5c-e2ef-415f-8f67-8c6f6374f6e5',\n",
       " 'e231a055-be74-46df-b70a-a3ece848c62d',\n",
       " '0414128a-aed4-4e9f-a84c-a8e8c13067bc',\n",
       " 'e922f8e5-4df5-471b-add1-57cac10f0e96',\n",
       " 'dbb2329e-ebe1-4e01-98be-563b70cfe29f',\n",
       " '6982ebbe-63e1-47fa-9699-ce3bd6781751',\n",
       " '423ea964-b455-4d0a-8461-3e9cb2dbaa83',\n",
       " '05c32ba2-8b72-4d5f-a672-7bbc6be99748',\n",
       " 'e1646ed0-d662-4a70-a806-7ec65359f1c1',\n",
       " 'af0fe243-7b18-4c34-8f64-11138ba5611b',\n",
       " '11d73459-d5f6-4ab7-acb6-dc2db9da5ace',\n",
       " 'e7f6c93d-1bf4-49dc-b60c-c5650d2a60f2',\n",
       " '6f73ec67-969a-4ce6-a8a5-d64ed36e95d5',\n",
       " '05e515a2-bd0b-4d8e-a699-513e067d54b0',\n",
       " '3b0857dc-d254-47ef-9d9a-cc6740d7503c',\n",
       " 'b6247e89-a942-47b3-97ef-5c35f6e45eec',\n",
       " '9929188e-722b-4a7d-b29a-73bcc7f8508e',\n",
       " 'f6cf3c88-8fbb-403d-a7f7-d9a4ed1d906f',\n",
       " '5e07dbec-b5bd-4882-a48b-16bf6735374f',\n",
       " '7568c437-0989-4a54-9270-dd2765cfe68b',\n",
       " 'eb22c803-01f2-4376-bfe6-54605a369164',\n",
       " 'b5600a99-fc9e-4233-b888-e99508c3d505',\n",
       " 'd179fd51-dcb5-4c48-91f6-81bc5fb05c41',\n",
       " '27d5e11e-10cc-47e0-ae7d-9dff0f711bee',\n",
       " 'e6a59693-0edd-423f-bee7-470a1fad014e',\n",
       " 'edef6a5d-aad2-4584-9fb0-8e10529f55e0',\n",
       " 'c9a65590-529c-4762-86bf-e75b830fc101']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 37}},\n",
       " 'total_vector_count': 37}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='27d5e11e-10cc-47e0-ae7d-9dff0f711bee', metadata={'page': 9.0, 'source': 'personal/paper-crash-detection.pdf'}, page_content='                                                                                                    Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\nJournal of IoT in Social, Mobile, Analytics, and Cloud, March 2024, Volume 6, Issue 1 63 \\n \\nThe facial detection system accurately identifies yawning and eye conditions as in \\nFigure 10, and the crash false notification triggering mechanism responds as depicted in Figure \\n7. \\n Conclusion \\nThe results obtained from the drowsiness and crash detection system demonstrate its \\neffectiveness in improving road safety. By detecting driver drowsiness and accurately \\nidentifying crash events while promptly notifying relevant parties, the system facilit ates swift \\nemergency response and aids in preventing potential accidents by monitoring the driver\\'s facial \\ncondition. Moreover, it promotes seamless coordination between vehicles and rescue center. \\nThe system\\'s accuracy rate and response time underscore its potential to enhance the efficiency \\nof emergency services and potentially reduce the severity of injuries in crash incidents. \\nNevertheless, it is important to acknowledge certain challenges and limitations. \\nEnvironmental factors, such as adverse weather conditions or heavy traffic congestion, may \\nintroduce additional complexities in crash detection. Further research efforts, such  as \\nintegrating both crash detection and drowsiness detection into a single device, are necessary to \\nenhance the system\\'s performance in such challenging scenarios. \\nReferences \\n[1] F. Bhatti, M. A. Shah, C. Maple, and S. U. Islam, \"A Novel Internet of Things-Enabled \\nAccident Detection and Reporting System for Smart City Environments,\" Sensors, vol. \\n19, no. 9, p. 2071, May 2019, doi: 10.3390/s19092071. \\n[2] S. Uma and R. Eswari, \"Accident prevention and safety assistance using IOT and \\nmachine learning,\" J. Reliab. Intell. Environ., vol. 8, no. 2, pp. 79–103, Jun. 2022, doi: \\n10.1007/s40860-021-00136-3. \\n[3] Pokhrel, Anisha, Laxmi Mahara, Monika Upadhyaya, Shikshya Shrestha, and Badri Raj \\nLamichhane. \"Drowsy Driver Detection with Crash Alert Mechanism using Arduino \\nand Image Processing.\" Journal of Soft Computing Paradigm 5, no. 2 (2023): 194-217. \\n[4] C. Prabha, R. Sunitha, and R. Anitha, \"Automatic Vehicle Accident Detection and \\nMessaging System Using GSM and GPS Modem,\" Int. J. Adv. Res. Electr. Electron. '),\n",
       " Document(id='7568c437-0989-4a54-9270-dd2765cfe68b', metadata={'page': 5.0, 'source': 'personal/paper-crash-detection.pdf'}, page_content=\"                                                                                                    Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\nJournal of IoT in Social, Mobile, Analytics, and Cloud, March 2024, Volume 6, Issue 1 59 \\n \\nFigure 3. Crash Detection Flow Chart \\n3.3 Components Used \\na.   Arduino UNO  \\nThe Arduino Uno, built around the ATmega328P microcontroller, receives velocity and \\ntilt values from the accelerometer [7]. It verifies if these values meet predefined conditions \\nbefore proceeding with further operations. \\nb.   MPU 6050 \\nIn this study, a 10 DOF IMU Sensor, capable of measuring 3 -axis accelerometer data, \\nis utilized to detect changes in a vehicle's orientation, indicative of potential accidents. To \\ncalculate velocity and prevent accidents, the acceleration -time relationship is  leveraged, \\nenabling accurate velocity estimation based on acceleration values over time \\n\"),\n",
       " Document(id='05c32ba2-8b72-4d5f-a672-7bbc6be99748', metadata={'page': 11.0, 'source': 'personal/paper-image-captioning.pdf'}, page_content=\"                                                                                                       Nabaraj Subedi, Nirajan Paudel, Manish Chhetri, Sudarshan Acharya, Nabin Lamichhane  \\n \\nJournal of Soft Computing Paradigm, Month 2024, Volume 6, Issue 1  81 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 9. Sample Result 3 \\n \\n \\n \\n \\n \\n \\n \\nFigure 10. Sample Result 4 \\nThe transformer model correctly predicted the object, environment, relations and the \\nactions in the images with coherent sentences. \\n Conclusion \\nIn conclusion, our research work introduces a novel approach for generating detailed \\nNepali paragraphs to describe images, leveraging both visual and linguistic struct ures. The \\ntransformer's capacity to model long range dependencies on caption to effectively describe the \\n\"),\n",
       " Document(id='1e1423b9-049e-4f7d-a57d-70a02819a2b6', metadata={'page': 2.0, 'source': 'personal/paper-image-captioning.pdf'}, page_content='Nepali Image Captioning: Generating Coherent Paragraph-Length Descriptions Using Transformer \\nISSN: 2582-2640  72 \\n \\n \\nThe key contributions of this research work are: \\n1. We compiled the Nepali Paragraph dataset for image captioning by manually refining \\ncaptions from the English Stanford dataset [1] and creating 800 original Nepali cultural \\nimage descriptions, while also verifying the accuracy of Google -translated content \\nthrough human correction. \\n2. Utilize a Transformer -CNN architecture to generate Nepali Paragraph caption s from \\nimages. Through our research, we aim to contribute to the advancement of image \\ncaptioning technology in Nepali, paving the way for improved accessibility, tourism \\nexperiences, and urban development initiatives in Nepal and beyond. \\n Related Works \\nUtilizing computer vision algorithms for image captioning has primarily been focused \\non English language datasets largely due to the inherent complexities of other languages. \\nHowever, the increasing need for multilingual image captioning has prompted researche rs to \\nexplore the extension of these techniques to languages beyond English. This expansion not \\nonly facilitates image-text retrieval but also enables image captioning and translation in diverse \\nlinguistic contexts. \\nAmong the foundational techniques used i n image captioning is the Long Short -Term \\nMemory (LSTM) neural network [8], renowned for its ability to maintain long -short term \\nmemory, thereby addressing the short -term memory limitations of standard Recurrent Neural \\nNetworks (RNNs). This feature is part icularly crucial for various tasks such as Natural \\nLanguage Processing (NLP), object detection, and machine translation. The prevailing \\nsequence translation models typically employ advanced convolutional and recurrent neural \\nnetworks organized in an encode r-decoder setup, often drawing inspiration from machine \\ntranslation methodologies. \\nIn the domain of non-English language image captioning, significant strides have been \\nmade, particularly in languages such as Hindi and Bengali, which share similarities wit h \\nNepali. In Bengali language research, notable studies by S. Paul et al. [9] have explored \\ntechniques utilizing convolutional neural networks (CNNs) and recurrent neural networks \\n(RNNs) to generate Bengali captions from images. Subsequent investigations b y the same ')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"gre\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def marks_reader(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Read the marks of subjects from the pdf and return subject marks\n",
    "    \"\"\"\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        text += pdf_reader.pages[page_num].extract_text()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [marks_reader]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are some key features and characteristics of C programming:\\n\\n1. **Portability**: C is a portable language, meaning that programs written in C can be compiled on different platforms with minimal modifications.\\n2. **Efficiency**: C is an efficient language, as it allows direct access to hardware resources such as memory and I/O devices.\\n3. **Flexibility**: C provides a wide range of data types, including integers, floating-point numbers, characters, and arrays.\\n4. **Control structures**: C has a variety of control structures, including if-else statements, switch statements, loops (for, while, do-while), and jump statements (goto).\\n5. **Functions**: C allows the definition of reusable blocks of code called functions, which can be called from anywhere in the program.\\n6. **Pointers**: C provides a powerful feature called pointers, which allow direct access to memory locations and are used extensively in system programming.\\n7. **Structures**: C allows the definition of custom data types using structures, which can be used to represent complex data such as objects or records.\\n8. **Arrays**: C provides support for arrays, which are collections of elements of the same type stored in contiguous memory locations.\\n9. **Memory management**: C requires manual memory management through functions like malloc() and free(), which can lead to memory leaks if not used carefully.\\n\\nSome common features of C programming include:\\n\\n* **Variable declarations**: Variables must be declared before use, and their data types are specified using keywords such as int, float, char, etc.\\n* **Assignment operators**: C provides a range of assignment operators, including =, +=, -=, *=, /=, etc.\\n* **Conditional statements**: C has if-else statements, switch statements, and logical operators (&&, ||, !) to control the flow of the program.\\n* **Loops**: C provides for loops (for), while loops, and do-while loops to iterate over code blocks.\\n\\nSome common pitfalls or challenges in C programming include:\\n\\n* **Memory leaks**: Failure to free allocated memory can lead to memory leaks, which can cause programs to consume increasing amounts of memory over time.\\n* **Buffer overflows**: Writing more data to a buffer than it is designed to hold can lead to unexpected behavior or crashes.\\n* **Null pointer dereferences**: Attempting to access memory through a null (non-existent) pointer can result in program crashes or undefined behavior.\\n\\nOverall, C programming requires attention to detail, careful use of pointers and memory management, and a good understanding of control structures and functions.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-10T14:37:40.213583Z', 'done': True, 'done_reason': 'stop', 'total_duration': 25547103792, 'load_duration': 828037333, 'prompt_eval_count': 35, 'prompt_eval_duration': 1872000000, 'eval_count': 521, 'eval_duration': 22841000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c665d5b7-84f6-4fed-b694-403aa267ec05-0', usage_metadata={'input_tokens': 35, 'output_tokens': 521, 'total_tokens': 556})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"can you tell me the marks of C programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
